{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "least-begin",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook records the measures taken to reproduce the 2019 paper by Naseri et al [1] as documented by the paper *Reproducing \"Analyzing and Predicting News Popularity in an Instant\n",
    "Messaging Service\"* by Vonderlind et al [2].\n",
    "\n",
    "*[1]: Mohammad Naseri and Hamed Zamani. 2019. Analyzing and Predicting News Popularity in an Instant Messaging Service. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). Association for Computing Machinery, New York, NY, USA, 1053–1056. DOI:https://doi.org/10.1145/3331184.3331301*  \n",
    "*[2]: Philip Vonderlind, Philipp Piwonka, and Julia Putz. 2022. Reproducing \"Analyzing and Predicting News Popularity in an Instant\n",
    "Messaging Service\". In .Zenodo, Vienna, Austria, 6 pages. https://doi.org/10.5281/zenodo.5920074*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-singapore",
   "metadata": {},
   "source": [
    "We will train separate models for each news agency Telegram channel, as described in the paper. Weirdly enough, the also added\n",
    "the Guardian, BCC Persian and the Washington Post in their dataset, but they\n",
    "are not mentioned in the paper ...\n",
    "\n",
    "We will collect a few measures also used in the paper and investiagte the results.\n",
    "\n",
    "As the first step, we will clone the dataset containing the Telegram data used by [Naseri 2019]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-luxury",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only if needed, e.g. when the Telegram News Folder is not yet downloaded!\n",
    "#! git clone https://github.com/IceCream71/TelegramNews.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-classic",
   "metadata": {},
   "source": [
    "**ISSUES FOUND**\n",
    "\n",
    "* **Which of the .bson files are actually used for the experiment?** There\n",
    "is no indication if Washington Post and BBC Persia etc. are used for the\n",
    "experiments, as they are not mentioned in the paper itself.\n",
    "\n",
    "* **How are the binary labels acutally generated from the data?** There is no indication for the threshold they used in the paper. They mention only: \"Therefore, we first found the thresholds that satisfy these percent-\n",
    "ages for training and test sets and assigned a binary label to each\n",
    "post (“popular” or “not popular”). \"\n",
    "\n",
    "* **How are the features generated from the data (manually, algorithm, ...)?** This would be important as the bson data does **not** have the same features. List of issues: What is the n-grams thing? How do we generate the hashtags and mention features? How are we supposed to generate the media-type features? What does Has Link mean (could be either the enitity type URL or a media page containing a url tag...).\n",
    "\n",
    "* **There is a typo in the features table** We can't really reproduce\n",
    "the #mentions features without guessing that they mean the count of the mentions\n",
    "in a post (They use the same text as for the hashtags)\n",
    "\n",
    "* **How are the NaN in the data handled?** Could be removed, zero-filled, interpolated. We just don't know...\n",
    "\n",
    "missing features (which can't be calculated due to missing information): \n",
    "\n",
    "* subscribers (shows the popularity of channel)\n",
    "* channel age (date of first post? in days)\n",
    "* frequent n-grams (frequent n-grams that news item contains)\n",
    "* frequent hashtags (frequent hashtags that news item contains)\n",
    "* frequent mentions (frequent mentions that news item contains)\n",
    "\n",
    "what does frequent mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bson\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.stats as st\n",
    "from scipy.stats._continuous_distns import _distn_names\n",
    "import warnings\n",
    "import csv\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from scipy import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_for_bson(path): \n",
    "  with open (path, 'rb') as f:\n",
    "    data = bson.decode_all(f.read())\n",
    "\n",
    "  df = pd.DataFrame(data)\n",
    "  df = handle_NAS(df)\n",
    "  df = extract_features(df)\n",
    "  df = cut_date_range(df)\n",
    "  df = drop_unused_features(df)\n",
    "  return df\n",
    "\n",
    "def handle_NAS(df):\n",
    "  df = df.dropna(subset=['views'],axis=0)\n",
    "  return df\n",
    "\n",
    "def extract_features(df):\n",
    "  df = create_datetime_features(df)\n",
    "  df = add_channel_features(df)\n",
    "  df = create_entity_features(df)\n",
    "  return df\n",
    "\n",
    "def create_datetime_features(df):\n",
    "  df['age'] =  df.date\n",
    "  df['date'] = pd.to_datetime(df.date, unit='s')\n",
    "  df['year'] = df.date.dt.year\n",
    "  df['month'] = df.date.dt.month\n",
    "  df['day'] = df.date.dt.day\n",
    "  df['weekday'] = df.date.dt.dayofweek\n",
    "  df['hour'] = df.date.dt.hour\n",
    "  return df\n",
    "\n",
    "def cut_date_range(df):\n",
    "  # March 8, 2017 to October 8, 2017.\n",
    "  start_date = \"2017-03-08\"\n",
    "  end_date  = \"2017-10-08\"\n",
    "  mask = (df.date > start_date) & (df.date <= end_date)\n",
    "  return df.loc[mask]\n",
    "\n",
    "def add_channel_features(df):\n",
    "  df['minViews'] = df.views.min()\n",
    "  df['maxViews'] = df.views.max()\n",
    "  df['meanViews'] = df.views.mean()\n",
    "  df['stdViews'] = df.views.std()\n",
    "  df['hourlyAvgPosts'] = df.groupby('hour').size().mean()\n",
    "  df['dailyAvgPosts'] = df.groupby(['year','month','day']).size().mean()\n",
    "  return df\n",
    "\n",
    "def create_entity_features(df):\n",
    "  # Convert the entity field from bson into the actual features.\n",
    "  # For each row, check if media exists, if yes set media type.\n",
    "  df['hasMedia'] = df.media.notnull()\n",
    "  df['mediaType'] = df.apply(lambda x: x.media['_'] if x.hasMedia else '', axis=1)\n",
    "  # Check if URL exists in entities, if exists, is a link\n",
    "  df['hasLink'] = df.apply(lambda x: True if type(x.entities) != float and [d for d in x.entities if d['_'] ==  'messageEntityTextUrl'] else False, axis=1)\n",
    "  # Collect the mentions from the entities feature\n",
    "  df['mentions'] = df.apply(lambda x: len([d for d in x.entities if d['_'] == 'messageEntityMention']) if type(x.entities) != float else 0, axis=1)\n",
    "  # Same as before for the hashtags\n",
    "  df['hashtags'] = df.apply(lambda x: len([d for d in x.entities if d['_'] == 'messageEntityHashtag']) if type(x.entities) != float else 0, axis=1)\n",
    "  return df\n",
    "\n",
    "def drop_unused_features(df):\n",
    "  return df.drop(labels=['_id','_','flags','post','id','to_id','reply_to_msg_id',\n",
    "                  'media','channel','entities','message','fwd_from','edit_date',\n",
    "                  'via_bot_id','reply_markup','action', 'silent'],axis=1,errors='ignore')\n",
    "\n",
    " \n",
    "reutersWorld = create_df_for_bson('./TelegramNews/mongo/telegram/ReutersWorld.bson')\n",
    "cnnBrk = create_df_for_bson('./TelegramNews/mongo/telegram/CNNBrk.bson')\n",
    "pressTV = create_df_for_bson('./TelegramNews/mongo/telegram/presstv.bson')\n",
    "bbcBreaking = create_df_for_bson('./TelegramNews/mongo/telegram/bbcbreaking.bson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-david",
   "metadata": {},
   "source": [
    "I added some tests belkow to check if my calculations and mappings of the features correspond to the ones reported in the paper. The results seem\n",
    "fairly accurate if we take rounding into account. \n",
    "\n",
    "What is weird is that\n",
    "the actual size of the datasets don't match up with the repoted dataset sizes\n",
    "in the paper. My guess would be that they just repoted their acutal test set\n",
    "sizes and not the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=cnnBrk.mentions,y=cnnBrk.index);\n",
    "print(f\"Mentions per news {round(cnnBrk.mentions.sum() / len(cnnBrk.index),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=cnnBrk.hashtags,y=cnnBrk.index);\n",
    "print(f\"Hastags per news {round(cnnBrk.hashtags.sum() / len(cnnBrk.index),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=cnnBrk.hasMedia,y=cnnBrk.index);\n",
    "print(f\"Media per news {round(len(cnnBrk[cnnBrk.hasMedia == True].index) / len(cnnBrk.index),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics_for_dataset(data, label):\n",
    "  news_item_count = len(data.index)\n",
    "  average_views = data.views.sum() / len(data.index)\n",
    "  std_views = np.std(data.views)\n",
    "  avg_hastags = round(data.hashtags.sum() / len(data.index),4)\n",
    "  avg_media = round(len(data[data.hasMedia == True].index) / len(data.index),4)\n",
    "  avg_mentions = round(data.mentions.sum() / len(data.index),4)\n",
    "  return {\"NI-count\": news_item_count,\n",
    "          \"avg_views\" : average_views,\n",
    "          \"std_views\" :std_views,\n",
    "          \"avg_hastags\" : avg_hastags,\n",
    "          \"avg_media\" : avg_media,\n",
    "          \"avg_mentions\" : avg_mentions,\n",
    "          \"agency\" : label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stat_block():\n",
    "  cnnStat = get_statistics_for_dataset(cnnBrk, \"CNN\")\n",
    "  bbcStat = get_statistics_for_dataset(bbcBreaking, \"BBC\")\n",
    "  presstvStat = get_statistics_for_dataset(pressTV , \"PressTV\")\n",
    "  reutersStat = get_statistics_for_dataset(reutersWorld, \"Reuters\")\n",
    "  res = pd.DataFrame([cnnStat, presstvStat, reutersStat, bbcStat])\n",
    "  return res\n",
    "generate_stat_block()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-dietary",
   "metadata": {},
   "source": [
    "# Distribution Assumptions\n",
    "We can now also test the distribution assumptions for the views variable made by the authors.\n",
    "For this we check all 89 distributions available in Skicit-Learn: (https://docs.scipy.org/doc/scipy/reference/stats.html)\n",
    "\n",
    "## Issues Found\n",
    "\n",
    "* **Which 89 distributions did they acutally test?** Scipy includes more distributions than 89 and is continually updated, so just linking to the page above is not enough ...\n",
    "\n",
    "* **Which metric was used to evaluate goodness of fit of the dist.?** No metric is repoted in the paper.\n",
    "\n",
    "* **Distributions do NOT MATCH with the paper !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_distribution(data, dist):\n",
    "    print(f\"Testing distribution {dist}\")\n",
    "    label = dist\n",
    "    dist = getattr(st, dist)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "      warnings.filterwarnings('ignore')\n",
    "      try :\n",
    "        params = dist.fit(data.views)\n",
    "\n",
    "        #parameters of fitted distribution.\n",
    "        arg = params[:-2]\n",
    "        loc = params[-2]\n",
    "        scale = params[-1]\n",
    "\n",
    "        #calc pdf and error for fit.\n",
    "        pdf = dist.pdf(np.arange(len(data.views)), loc=loc, scale=scale, *arg)\n",
    "        sse = np.sum(np.power(data.views - pdf, 2.0))\n",
    "\n",
    "        #calc Kolgorov-Smirnov\n",
    "        dist, p = st.kstest(data.views, label, args=params)\n",
    "      except ValueError:\n",
    "        print(f\"Could not process distribution {dist}\")\n",
    "        return False\n",
    "    return {\"name\" : label, \"params\" : params, \"Error\" : sse, \"P-Value\" : p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_distribution(data):\n",
    "  results = []\n",
    "  # Exclude levy_stable and studentized_range as their implementations in \n",
    "  # scipy suck and take a vast amount of time to compute.\n",
    "  dist_to_test = [d for d in _distn_names if not d in ['levy_stable', 'studentized_range']]\n",
    "  for dist in dist_to_test:\n",
    "    res = test_distribution(data, dist)\n",
    "    if res:\n",
    "      results.append(res)\n",
    "  return sorted(results, key=lambda x:x[\"P-Value\"])\n",
    "\n",
    "def generate_and_save_distribution_tests_for(data, label):\n",
    "  dists = test_best_distribution(data)\n",
    "  cols = [\"name\", \"params\", \"Error\", \"P-Value\"]\n",
    "  with open(f'distributions/{label}.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=cols)\n",
    "    writer.writeheader()\n",
    "    for d in dists:\n",
    "      writer.writerow(d)\n",
    "  print(f\"Done processing distribution tests for {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_distribution_tests_for(cnnBrk, \"cnn_breaking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_distribution_tests_for(reutersWorld, \"reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_distribution_tests_for(bbcBreaking, \"bbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-firmware",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_and_save_distribution_tests_for(pressTV, \"presstv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-comedy",
   "metadata": {},
   "source": [
    "# Label into top 5% and top 25%\n",
    "\n",
    "## Issues Found\n",
    "\n",
    "* **How are the labels applied?** My guess would be via a categorical variable as target, which is also what is applied below.\n",
    "\n",
    "* **Are there separate datasets for 25% and 5%?** I would guess this should be the case, since they mention _\"Therefore, we first found the thresholds that satisfy these percent-\n",
    "ages for training and test sets and assigned a binary label to each\n",
    "post (“popular” or “not popular”). \"_ I would say this suggest they do the experiments two times, once for each popularity prediction percentge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_binary_labels(df):\n",
    "  sorted = df.sort_values(by='views', ascending=False)\n",
    "  top5_indices = sorted.head(int(len(df)*(5/100))).index\n",
    "  top25_indices = sorted.head(int(len(df)*(25/100))).index\n",
    "\n",
    "  # Create the target, 0 for not popular, 1 for popular\n",
    "  df.loc[:,'popularity'] = 0\n",
    "\n",
    "  top5_set = df.copy()\n",
    "  top25_set = df.copy()\n",
    "\n",
    "  top5_set.loc[top5_indices,'popularity'] = 1\n",
    "  top25_set.loc[top25_indices,'popularity'] = 1\n",
    "\n",
    "  return top5_set, top25_set\n",
    "\n",
    "cnnBrk5, cnnBrk25 = produce_binary_labels(cnnBrk)\n",
    "sns.barplot(x=[0,1], y=cnnBrk5.groupby('popularity').popularity.count());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=[0,1], y=cnnBrk25.groupby('popularity').popularity.count());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-northern",
   "metadata": {},
   "source": [
    "# Split into Training/Test\n",
    "\n",
    "## Issues Found\n",
    "\n",
    "* **Maybe there are imbalances in train/test due to holidays or \n",
    "interesting news topics in those months?** (Just a thought)\n",
    "\n",
    "* **Very few training samples!** The authors use 86.65% of their data for training, leaving just shy of 14% for testing. That may be an issue in case of overfitting.\n",
    "\n",
    "* **They don't mention which columns are scaled!!** This results in static cols,\n",
    "such as the std of views etc. to get put to 0!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(df):\n",
    "  # the first six months of the data is selected for training and the last month \n",
    "  # is selected for testing.\n",
    "   # March 8, 2017 to October 8, 2017.\n",
    "  train_start = \"2017-03-08\"\n",
    "  train_end = \"2017-9-08\"\n",
    "  mask = (df.date > train_start) & (df.date <= train_end)\n",
    "\n",
    "  df = df.drop(labels=['date'],axis=1)\n",
    "  df.mediaType = df.mediaType.astype('category').cat.codes\n",
    "\n",
    "  training = df.loc[mask]\n",
    "  test = df.loc[-mask]\n",
    "\n",
    "  # Apply Min-Max scaling fitted on the training data\n",
    "  scaler = MinMaxScaler()\n",
    "  scaled_training = pd.DataFrame(scaler.fit_transform(training), columns=df.columns)\n",
    "  scaled_test = pd.DataFrame(scaler.transform(test), columns=df.columns)\n",
    "\n",
    "  return scaled_training, scaled_test\n",
    "\n",
    "cnnTrain5, cnnTest5 = create_train_test_split(cnnBrk5)\n",
    "print(f\"Size of Training set: {len(cnnTrain5.index)}\")\n",
    "print(f\"Size of Test set: {len(cnnTest5.index)}\")\n",
    "print(f\"Percentage of train split: {round(100 * len(cnnTrain5.index) / len(cnnBrk.index),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnTrain25, cnnTest25 = create_train_test_split(cnnBrk25)\n",
    "print(f\"Size of Training set: {len(cnnTrain25.index)}\")\n",
    "print(f\"Size of Test set: {len(cnnTest25.index)}\")\n",
    "print(f\"Percentage of train split: {round(100 * len(cnnTrain25.index) / len(cnnBrk.index),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-theta",
   "metadata": {},
   "source": [
    "# SVM Partial Models (STL)\n",
    "\n",
    "We will follow the paper and find the tuning parameter **c** by applying **5-fold CV**. We will collect the same 5 metrics mentioned in the paper. \n",
    "\n",
    "We also also try feature selection using randomized Lasso, as mentioned in the paper. Should their assumptions hold, the results should be that no substantial improvement can be shown.\n",
    "\n",
    "Before training we also have to also apply the sample weighting technique mentioned for the MTL: _\"Both of these models\n",
    "also use a sample weighting technique, exactly the same as the one\n",
    "used for MTL.\"_\n",
    "\n",
    "## Issues Found\n",
    "\n",
    "* **SVM also has other hyperparameters that they don't mention!!** These include gamma and kernel (and degree for some kernels). As they don't mention these parameters, we will just assume the default is used.\n",
    "\n",
    "* **Not mentioned wether they do GridSearch of Randomized Search for the hyperparameter tuning!**\n",
    "\n",
    "* **Which range of parameters for c is the search based on?** They only mention that they use 5-fold CV, but not over which range of parameters. We will just have to take some arbitrary values and hope for the best.\n",
    "\n",
    "* **Which metric is used to decide which CV fold is the best?** E.g. which\n",
    "scorer should be used to determine the optimal 'C' hyperparamter.\n",
    "\n",
    "* **Which Kernel do they use?** Since they use linear normalization, I guess one\n",
    "would use a linear kernel? This would also make the computation much faster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(train_df):\n",
    "  n = len(train_df.index)\n",
    "  labels = train_df.popularity\n",
    "  popular = len(labels[labels == 1])\n",
    "  not_popular = n - popular\n",
    "\n",
    "  denominator = sum((1 / (popular if labels[k] == 1 else not_popular)) for k in range(1,n))\n",
    "  bigLambda = map(lambda i: (1 / (popular if labels[i] == 1 else not_popular)) * denominator, range(0, n))\n",
    "  return pd.Series(bigLambda)\n",
    "\n",
    "bigLambdaCnn25 = calculate_weights(cnnTrain25)\n",
    "bigLambdaCnn25.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_wo_feature_select(train_df, weights):\n",
    "  weight_array = weights.unique()\n",
    "  weight_dict = {0 : weight_array[0], 1 : weight_array[1]}\n",
    "  param_grid = {'C' : np.logspace(-3, 2, 6)}\n",
    "\n",
    "  # Do 5-fold CV\n",
    "  # Use LinearSVC as we get quadratic scaling for the standard RFB\n",
    "  # kernel otherwise, which is unreasonable given the amount of data.\n",
    "  gridSearch = GridSearchCV(LinearSVC(class_weight=weight_dict), param_grid,\n",
    "                            verbose=1) \n",
    "  model = gridSearch.fit(train_df.drop('popularity', axis=1), \n",
    "                         train_df.popularity)\n",
    "  print(f\"Found best model :{model.best_params_}\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_for_model(model, test_df):\n",
    "  # Get predictions from model\n",
    "  test_preds = model.predict(test_df.drop('popularity', axis=1))\n",
    "  test_accuracy = accuracy_score(test_df.popularity, test_preds)\n",
    "  test_balanced_acc = balanced_accuracy_score(test_df.popularity, test_preds)\n",
    "  test_f1 = f1_score(test_df.popularity, test_preds)\n",
    "  test_precision = precision_score(test_df.popularity, test_preds)\n",
    "  test_recall = recall_score(test_df.popularity, test_preds)\n",
    "\n",
    "  print(\"Confusion Matrix\".center(50,\"-\"))\n",
    "  ConfusionMatrixDisplay.from_estimator(model, test_df.drop('popularity', axis=1), test_df.popularity)\n",
    "  plt.show()\n",
    "  \n",
    "  return {\"Accuracy\" : test_accuracy, \"Balanced_Accuracy\": test_balanced_acc, \n",
    "          \"Precision\": test_precision, \"Recall\": test_recall, \"F1\": test_f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-violation",
   "metadata": {},
   "source": [
    "# Evaluation of model + generation for other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_for_dataset(df, name):\n",
    "  print(name.center(50,\"=\"))\n",
    "\n",
    "  # Create labbeled train test splits for 5% and 25%\n",
    "  train, test = create_train_test_split(df)\n",
    "  train5, train25 = produce_binary_labels(train)\n",
    "  test5, test25 = produce_binary_labels(test)\n",
    "\n",
    "  # Train models for 5% and 25%\n",
    "  results_5 = get_model_results(train5, test5)\n",
    "  results_25 = get_model_results(train25, test25)\n",
    "\n",
    "  # Save best models to disk\n",
    "  with open(f'{name}_5.pkl', 'wb') as fid:\n",
    "    pickle.dump(results_5['model'], fid)  \n",
    "\n",
    "  with open(f'{name}_25.pkl', 'wb') as fid:\n",
    "    pickle.dump(results_25['model'], fid)  \n",
    "  \n",
    "  print(f\"Results for dataset {name}:\")\n",
    "  print(f\"5% Popularity: {results_5['metrics']}\")\n",
    "  print(f\"25% Popularity: {results_25['metrics']}\")\n",
    "  return {\"5\" : results_5, \"25\" : results_25}\n",
    "\n",
    "def get_model_results(train, test):\n",
    "  train_weights = calculate_weights(train)\n",
    "  model = train_svm_wo_feature_select(train, train_weights)\n",
    "  metrics = calculate_metrics_for_model(model, test)\n",
    "  return {\"model\" : model, \"metrics\" : metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-hundred",
   "metadata": {},
   "source": [
    "## Reuters World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "resReuters = get_results_for_dataset(reutersWorld, \"Reuters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-shelf",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "resCNN = get_results_for_dataset(cnnBrk, \"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-weekend",
   "metadata": {},
   "source": [
    "## Press TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "resPressTV = get_results_for_dataset(pressTV, \"PressTV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-melbourne",
   "metadata": {},
   "source": [
    "## BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "resBBC = get_results_for_dataset(bbcBreaking, \"BBC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-sister",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The results seem similar for some of the models, but not very good for most metrics. This is probably due to inconsistencies mentioned above caused by\n",
    "the authors leaving out critical information about the algorithm's hyperparamters and the pre-processing of the dataset.\n",
    "\n",
    "From my guesses I would also say that we don't have a model that fits the\n",
    "data good enough. Maybe it isn't linearily seperable. Henceforth, a simple\n",
    "LinearSVC does not suffice. \n",
    "\n",
    "Another probable issue is the massive imbalance between training and test samples. Test sets are small to begin with, and by labelling the data using\n",
    "only 5% and 25% of the data, we reduce the quantity of popular news to \n",
    "next to nothing. For this reason, most classifiers on 5% seem to just overfit\n",
    "the Unpopular News (label 0) class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-vessel",
   "metadata": {},
   "source": [
    "# MALSAR Multitask CASO (MTL)\n",
    "\n",
    "Next we try to reproduce the MTL performance measures by using the MALSAR package. Since we work with an external package, we need to split and evaluate the data in a slightly different order. Keep in mind the train/test split is deterministic, and therefore the datasets are comparable to the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnTrain, cnnTest = create_train_test_split(cnnBrk)\n",
    "cnnTrain5, cnnTrain25 = produce_binary_labels(cnnTrain)\n",
    "cnnTest5, cnnTest25 = produce_binary_labels(cnnTest)\n",
    "\n",
    "reuTrain, reuTest = create_train_test_split(reutersWorld)\n",
    "reuTrain5, reuTrain25 = produce_binary_labels(reuTrain)\n",
    "reuTest5, reuTest25 = produce_binary_labels(reuTest)\n",
    "\n",
    "bbcTrain, bbcTest = create_train_test_split(bbcBreaking)\n",
    "bbcTrain5, bbcTrain25 = produce_binary_labels(bbcTrain)\n",
    "bbcTest5, bbcTest25 = produce_binary_labels(bbcTest)\n",
    "\n",
    "preTrain, preTest = create_train_test_split(pressTV)\n",
    "preTrain5, preTrain25 = produce_binary_labels(preTrain)\n",
    "preTest5, preTest25 = produce_binary_labels(preTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-yahoo",
   "metadata": {},
   "source": [
    "To prepare our data for MATLAB, we must split apart the X and Y matrices for each channel and popularity label type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df, nm = False):\n",
    "    weight = calculate_weights(df) # where to apply?\n",
    "                                   \n",
    "    df.loc[df.popularity == 0, \"popularity\"] = -1\n",
    "    X = df.drop(\"popularity\", axis=1)\n",
    "    Y = df.popularity\n",
    "    \n",
    "    if nm:\n",
    "        X.to_csv(\"X_\"+nm+\".csv\", header=False, index=False)\n",
    "        Y.to_csv(\"Y_\"+nm+\".csv\", header=False, index=False)\n",
    "        \n",
    "    return X,Y\n",
    "\n",
    "X_cnnTrain25, Y_cnnTrain25 = split_xy(cnnTrain25, 'cnn25')\n",
    "X_reuTrain25, Y_reuTrain25 = split_xy(reuTrain25, 'reu25')\n",
    "X_bbcTrain25, Y_bbcTrain25 = split_xy(bbcTrain25, 'bbc25')\n",
    "X_preTrain25, Y_preTrain25 = split_xy(preTrain25, 'pre25')\n",
    "\n",
    "X_cnnTest25, Y_cnnTest25 = split_xy(cnnTest25)\n",
    "X_reuTest25, Y_reuTest25 = split_xy(reuTest25)\n",
    "X_bbcTest25, Y_bbcTest25 = split_xy(bbcTest25)\n",
    "X_preTest25, Y_preTest25 = split_xy(preTest25)\n",
    "\n",
    "X_cnnTrain5, Y_cnnTrain5 = split_xy(cnnTrain5, 'cnn5')\n",
    "X_reuTrain5, Y_reuTrain5 = split_xy(reuTrain5, 'reu5')\n",
    "X_bbcTrain5, Y_bbcTrain5 = split_xy(bbcTrain5, 'bbc5')\n",
    "X_preTrain5, Y_preTrain5 = split_xy(preTrain5, 'pre5')\n",
    "\n",
    "X_cnnTest5, Y_cnnTest5 = split_xy(cnnTest5)\n",
    "X_reuTest5, Y_reuTest5 = split_xy(reuTest5)\n",
    "X_bbcTest5, Y_bbcTest5 = split_xy(bbcTest5)\n",
    "X_preTest5, Y_preTest5 = split_xy(preTest5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-translation",
   "metadata": {},
   "source": [
    "**Caution:** To proceed, the model parameters should be generated by executing edds.m. Please keep in mind the MALSAR package must be installed as laid out in the manual. For quick re-execution, the model files are already included in the repository.\n",
    "\n",
    "Once the MATLAB module has been executed, the following code will reimport the calculated model coefficients and apply it to our train and test sets for evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_for_model(res_df, nm): \n",
    "    test_accuracy = accuracy_score(res_df.original, res_df.predicted)\n",
    "    test_balanced_acc = balanced_accuracy_score(res_df.original, res_df.predicted)\n",
    "    test_f1 = f1_score(res_df.original, res_df.predicted)\n",
    "    test_precision = precision_score(res_df.original, res_df.predicted)\n",
    "    test_recall = recall_score(res_df.original, res_df.predicted)\n",
    "\n",
    "    print((nm+\" Confusion Matrix\").center(50,\"-\"))\n",
    "    cm = confusion_matrix(res_df.original, res_df.predicted)\n",
    "    ConfusionMatrixDisplay(cm).plot()\n",
    "    plt.show()\n",
    "\n",
    "    print({\"Accuracy\" : test_accuracy, \"Balanced_Accuracy\": test_balanced_acc, \n",
    "          \"Precision\": test_precision, \"Recall\": test_recall, \"F1\": test_f1})\n",
    "\n",
    "def evaluate_mlt(X, Y, W, c, i, nm):\n",
    "    \n",
    "    X = X.copy()\n",
    "    X.columns = range(0,X.shape[1])\n",
    "    res = np.sign(X.dot(W[i]) + c[i])\n",
    "\n",
    "    #display(res)\n",
    "    df_res = pd.DataFrame(data={'original':Y,'predicted':res})\n",
    "    #display(df_res.loc[df_res[\"original\"]==1,:])\n",
    "    #display(df_res.loc[df_res[\"original\"]==-1,:])\n",
    "    \n",
    "    display(pd.crosstab(df_res[\"original\"], df_res[\"predicted\"]))\n",
    "    \n",
    "    calculate_metrics_for_model(df_res, nm)\n",
    "    \n",
    "\n",
    "W = pd.read_csv('train_w25.csv',header=None)\n",
    "c = np.genfromtxt('train_c25.csv',delimiter=',')\n",
    "    \n",
    "evaluate_mlt(X_cnnTrain25, Y_cnnTrain25, W, c, 0, 'CNN Train25')\n",
    "evaluate_mlt(X_reuTrain25, Y_reuTrain25, W, c, 1, 'Reuters Train25')\n",
    "evaluate_mlt(X_bbcTrain25, Y_bbcTrain25, W, c, 2, 'BBC Train25')\n",
    "evaluate_mlt(X_preTrain25, Y_preTrain25, W, c, 3, 'PressTV Train25')\n",
    "\n",
    "evaluate_mlt(X_cnnTest25, Y_cnnTest25, W, c, 0, 'CNN Test25')\n",
    "evaluate_mlt(X_reuTest25, Y_reuTest25, W, c, 1, 'Reuters Test25')\n",
    "evaluate_mlt(X_bbcTest25, Y_bbcTest25, W, c, 2, 'BBC Test25')\n",
    "evaluate_mlt(X_preTest25, Y_preTest25, W, c, 3, 'PressTV Test25')\n",
    "\n",
    "W = pd.read_csv('train_w5.csv',header=None)\n",
    "c = np.genfromtxt('train_c5.csv',delimiter=',')\n",
    "\n",
    "evaluate_mlt(X_cnnTrain5, Y_cnnTrain5, W, c, 0, 'CNN Train5')\n",
    "evaluate_mlt(X_reuTrain5, Y_reuTrain5, W, c, 1, 'Reuters Train5')\n",
    "evaluate_mlt(X_bbcTrain5, Y_bbcTrain5, W, c, 2, 'BBC Train5')\n",
    "evaluate_mlt(X_preTrain5, Y_preTrain5, W, c, 3, 'PressTV Train5')\n",
    "\n",
    "evaluate_mlt(X_cnnTest5, Y_cnnTest5, W, c, 0, 'CNN Test5')\n",
    "evaluate_mlt(X_reuTest5, Y_reuTest5, W, c, 1, 'Reuters Test5')\n",
    "evaluate_mlt(X_bbcTest5, Y_bbcTest5, W, c, 2, 'BBC Test5')\n",
    "evaluate_mlt(X_preTest5, Y_preTest5, W, c, 3, 'PressTV Test5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-moderator",
   "metadata": {},
   "source": [
    "# SVM Full model (STL-All)\n",
    "\n",
    "Within this chapter the single task learning model, **STL-All** (SVM classifier trained on all the data from all channels) is implemented based on the Paper Analyzing and Predicting News Popularity in an Instant Messaging Service. It is checked whether or not the implementation and the results reported in the paper are reproducible and what information might be missing or wrong.\n",
    "\n",
    "Note that preprocessing needs to be slightly adapted to include channel name and standardize on a per-channel basis. Resulting datasets are therefore comparable to STL and MALSAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_channel(df, channel):\n",
    "    df['channel']=channel\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_df_for_bson(path, channel):\n",
    "  with open (path, 'rb') as f:\n",
    "    data = bson.decode_all(f.read())\n",
    "\n",
    "  df = pd.DataFrame(data)\n",
    "  df = handle_NAS(df)\n",
    "  df = extract_features(df)\n",
    "  df = cut_date_range(df)\n",
    "  df = drop_unused_features(df)\n",
    "  df = add_channel(df, channel)\n",
    "  return df\n",
    "\n",
    "\n",
    "# keep needed features, more flexible as different datasets contain different features\n",
    "def drop_unused_features(df):\n",
    "  return df.drop(df.columns.difference(['age', 'date', 'year', 'month', 'day', 'weekday', \n",
    "                          'hour', 'minViews', 'maxViews', 'meanViews', 'stdViews',\n",
    "                          'hasMedia', 'mediaType', 'hasLink', 'mentions', 'hashtags', 'views']),axis=1)\n",
    "\n",
    " \n",
    "reutersWorld = create_df_for_bson('./TelegramNews/mongo/telegram/ReutersWorld.bson', 'reutersWorld')\n",
    "cnnBrk = create_df_for_bson('./TelegramNews/mongo/telegram/CNNBrk.bson', 'cnnBrk')\n",
    "pressTV = create_df_for_bson('./TelegramNews/mongo/telegram/presstv.bson', 'pressTV')\n",
    "bbcBreaking = create_df_for_bson('./TelegramNews/mongo/telegram/bbcbreaking.bson', 'bbc')\n",
    "bbcPersian = create_df_for_bson('./TelegramNews/mongo/telegram/bbcpersian.bson', 'bbc')\n",
    "washingtonPost = create_df_for_bson('./TelegramNews/mongo/telegram/washingtonpost.bson', 'washingtonPost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-envelope",
   "metadata": {},
   "source": [
    "In the paper it is not clearly stated which datasets are included in training SVM all. In the repository we found more datasets than the ones listed in the table comparing the different models. Hence we simply assumed that for training SVM all, all available datasets (except the Guardian as it doesn't contain data) are used and only the ones listed in the mentioned table are used for prediction.\n",
    "\n",
    "Also they predicted BBC, however two datasets called bbcBreaking and bbcPersian do exist. It is not clear if BBC simply combines these two, or if only one of them is chosen as BBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [reutersWorld, cnnBrk, bbcBreaking, bbcPersian, pressTV, washingtonPost]\n",
    "all = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-papua",
   "metadata": {},
   "source": [
    "We assumed that the train and test split is done first according to *Therefore, we first found the thresholds that satisfy these percentages for training and test sets and assigned a binary label to each post*. Also, if the split is done after assigning the popularity, in e.g. the cnn test set no popular posts can be found and the prediction would have an accuracy of 1.0, which is not the case in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(df):\n",
    "  # the first six months (March - September) of the data is selected for training and the last month (October)\n",
    "  # is selected for testing.\n",
    "   # March 8, 2017 to September 7, 2017.\n",
    "  train_start = \"2017-03-08\"\n",
    "  train_end = \"2017-09-07\"\n",
    "  mask = (df.date > train_start) & (df.date <= train_end)\n",
    "\n",
    "  df = df.drop(labels=['date'],axis=1)\n",
    "  df.mediaType = df.mediaType.astype('category').cat.codes\n",
    "\n",
    "  training = df.loc[mask]\n",
    "  test = df.loc[-mask]\n",
    "\n",
    "  return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrain, allTest = create_train_test_split(all)\n",
    "\n",
    "print(f\"Size of Training set: {len(allTrain.index)}\")\n",
    "print(f\"Size of Test set: {len(allTest.index)}\")\n",
    "print(f\"Percentage of train split: {round(100 * len(allTrain.index) / len(all.index),2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-exclusive",
   "metadata": {},
   "source": [
    "It is not clearly stated in the paper how the threshold which is used to state whether a post is popular or not is defined. In this case it is assumed, that the more views a post has, the more popular it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# popular = 1\n",
    "# not popular = 0\n",
    "\n",
    "def assign_popularity (df, percent):\n",
    "  df.sort_values(by=['views'], inplace=True, ascending=False)\n",
    "  \n",
    "  quantile = (100-percent)/100\n",
    "  threshold = df.loc[:,'views'].quantile(quantile)\n",
    "  is_popular = df.loc[:,'views'] > threshold\n",
    "\n",
    "  df.loc[is_popular,'Popular']=1\n",
    "  df.loc[~is_popular, 'Popular']=0\n",
    "\n",
    "  return df\n",
    "\n",
    "allTrain_5 = assign_popularity(allTrain, 5)\n",
    "allTest_5 = assign_popularity(allTest, 5)\n",
    "\n",
    "allTrain_25 = assign_popularity(allTrain, 25)\n",
    "allTest_25 = assign_popularity(allTest, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allTrain_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Min-Max scaling fitted on the training data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "#training\n",
    "scaled_training_5 = pd.DataFrame(scaler.fit_transform(allTrain_5.drop('channel', axis=1)), columns=allTrain_5.drop('channel', axis=1).columns)\n",
    "scaled_training_25 = pd.DataFrame(scaler.fit_transform(allTrain_25.drop('channel', axis=1)), columns=allTrain_25.drop('channel', axis=1).columns)\n",
    "#test\n",
    "scaled_test_5 = pd.DataFrame(scaler.transform(allTest_5.drop('channel', axis=1)), columns=allTest_5.drop('channel', axis=1).columns)\n",
    "scaled_test_25 = pd.DataFrame(scaler.transform(allTest_25.drop('channel', axis=1)), columns=allTest_25.drop('channel', axis=1).columns)\n",
    "\n",
    "\n",
    "def scale_partial(df, channel):\n",
    "  is_channel = df.loc[:, 'channel']==channel\n",
    "  channel_df = df.loc[is_channel]\n",
    "  return pd.DataFrame(scaler.transform(channel_df.drop('channel', axis=1)), \n",
    "                      columns=channel_df.drop('channel', axis=1).columns)\n",
    "\n",
    "cnn_scaled_test_5 = scale_partial(allTest_5, 'cnnBrk')\n",
    "reuters_scaled_test_5 = scale_partial(allTest_5, 'reutersWorld')\n",
    "press_scaled_test_5 = scale_partial(allTest_5, 'pressTV')\n",
    "bbc_scaled_test_5 = scale_partial(allTest_5, 'bbc')\n",
    "\n",
    "cnn_scaled_test_25 = scale_partial(allTest_25, 'cnnBrk')\n",
    "reuters_scaled_test_25 = scale_partial(allTest_25, 'reutersWorld')\n",
    "press_scaled_test_25 = scale_partial(allTest_25, 'pressTV')\n",
    "bbc_scaled_test_25 = scale_partial(allTest_25, 'bbc')\n",
    "\n",
    "print(scaled_training_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-invite",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "Aim is to predict top 5% and top 25% popular news for each agency. Therefore train svm classifier on all channels and predict popular news for only one **channel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(train_df):\n",
    "  n = len(train_df.index)\n",
    "  labels = train_df.Popular\n",
    "  popular = len(labels[labels == 1])\n",
    "  not_popular = n - popular\n",
    "\n",
    "  denominator = sum((1 / (popular if labels[k] == 1 else not_popular)) for k in range(1,n))\n",
    "  bigLambda = map(lambda i: (1 / (popular if labels[i] == 1 else not_popular)) * denominator, range(0, n))\n",
    "  return pd.Series(bigLambda)\n",
    "\n",
    "weights_5 = calculate_weights(scaled_training_5)\n",
    "weights_25 = calculate_weights(scaled_training_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-sound",
   "metadata": {},
   "source": [
    "### Top 5%\n",
    "Interestingly using GridSearch including the weights as stated in the paper, the results are way more different than simply applying svc (using default settings). If adding the weights to the simple svc implementation, the results turn out to be exactly the same as when using the gridSearch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "y5 = scaled_training_5['Popular']\n",
    "x5 = scaled_training_5.drop('Popular', axis=1)\n",
    "\n",
    "param_grid = {'C' : np.logspace(-3, 2, 6)}\n",
    "weight_array = weights_5.unique()\n",
    "weight_dict = {0 : weight_array[0], 1 : weight_array[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(x5, y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier_grid = GridSearchCV(svm.SVC(), param_grid=param_grid)\n",
    "clf_grid = svc_classifier_grid.fit(x5,y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier_grid = GridSearchCV(svm.SVC(class_weight=weight_dict), param_grid=param_grid, n_jobs=8)\n",
    "clf_grid_weighted = svc_classifier_grid.fit(x5,y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier_grid = GridSearchCV(svm.LinearSVC(), cv=5, param_grid=param_grid, n_jobs=8)\n",
    "clf_grid_linear = svc_classifier_grid.fit(x5,y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier_grid = GridSearchCV(svm.LinearSVC(class_weight=weight_dict), param_grid=param_grid, cv=5, n_jobs=8)\n",
    "clf_grid_weighted_linear = svc_classifier_grid.fit(x5,y5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-genealogy",
   "metadata": {},
   "source": [
    "#### Prediction + Evaluation of Top5% models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(true, predicted):\n",
    "  acc = accuracy_score(true, predicted)\n",
    "  ba = balanced_accuracy_score(true, predicted)\n",
    "  prec = precision_score(true, predicted)\n",
    "  recall = recall_score(true, predicted)\n",
    "  f1 = f1_score(true, predicted)\n",
    "\n",
    "  print('Accuracy: ', acc)\n",
    "  print('Balanced Accuracy: ', ba)\n",
    "  print('Precision: ', prec)\n",
    "  print('Recall: ', recall)\n",
    "  print('F1: ', f1)\n",
    "\n",
    "  return confusion_matrix(true, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-saver",
   "metadata": {},
   "source": [
    "Prediction for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y5_cnn_test = cnn_scaled_test_5['Popular']\n",
    "x5_cnn_test = cnn_scaled_test_5.drop('Popular', axis=1)\n",
    "\n",
    "cnn_default = clf.predict(x5_cnn_test)\n",
    "print('clf - default settings')\n",
    "print(model_metrics(y5_cnn_test, cnn_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_grid = clf_grid.predict(x5_cnn_test)\n",
    "print('grid - default settings')\n",
    "print(model_metrics(y5_cnn_test, cnn_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_grid_weighted = clf_grid_weighted.predict(x5_cnn_test)\n",
    "print('grid - weighted')\n",
    "print(model_metrics(y5_cnn_test, cnn_grid_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_grid_linear = clf_grid_linear.predict(x5_cnn_test)\n",
    "print('grid - linear')\n",
    "print(model_metrics(y5_cnn_test, cnn_grid_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_grid_linear_weighted = clf_grid_weighted_linear.predict(x5_cnn_test)\n",
    "print('grid - linear + weighted')\n",
    "print(model_metrics(y5_cnn_test, cnn_grid_linear_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-congo",
   "metadata": {},
   "source": [
    "Prediction for BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "y5_bbc_test = bbc_scaled_test_5['Popular']\n",
    "x5_bbc_test = bbc_scaled_test_5.drop('Popular', axis=1)\n",
    "\n",
    "bbc_default = clf.predict(x5_bbc_test)\n",
    "print('clf - default settings')\n",
    "print(model_metrics(y5_bbc_test, bbc_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_grid = clf_grid.predict(x5_bbc_test)\n",
    "print('grid - default settings')\n",
    "print(model_metrics(y5_bbc_test, bbc_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_grid_weighted = clf_grid_weighted.predict(x5_bbc_test)\n",
    "print('grid - weighted')\n",
    "print(model_metrics(y5_bbc_test, bbc_grid_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_grid_linear_weighted = clf_grid_weighted_linear.predict(x5_bbc_test)\n",
    "print('grid - linear + weighted')\n",
    "print(model_metrics(y5_bbc_test, bbc_grid_linear_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-sending",
   "metadata": {},
   "source": [
    "Prediction for Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "y5_reuters_test = reuters_scaled_test_5['Popular']\n",
    "x5_reuters_test = reuters_scaled_test_5.drop('Popular', axis=1)\n",
    "\n",
    "reuters_default = clf.predict(x5_reuters_test)\n",
    "print('clf - default settings')\n",
    "print(model_metrics(y5_reuters_test, reuters_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_grid = clf_grid.predict(x5_reuters_test)\n",
    "print('grid - default settings')\n",
    "print(model_metrics(y5_reuters_test, reuters_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_grid_weighted = clf_grid_weighted.predict(x5_reuters_test)\n",
    "print('grid - weighted')\n",
    "print(model_metrics(y5_reuters_test, reuters_grid_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_grid_linear_weighted = clf_grid_weighted_linear.predict(x5_reuters_test)\n",
    "print('grid - linear + weighted')\n",
    "print(model_metrics(y5_reuters_test, reuters_grid_linear_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-philosophy",
   "metadata": {},
   "source": [
    "Prediction for PressTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "y5_press_test = press_scaled_test_5['Popular']\n",
    "x5_press_test = press_scaled_test_5.drop('Popular', axis=1)\n",
    "\n",
    "press_default = clf.predict(x5_press_test)\n",
    "print('clf - default settings')\n",
    "print(model_metrics(y5_press_test, press_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_grid = clf_grid.predict(x5_press_test)\n",
    "print('grid - default settings')\n",
    "print(model_metrics(y5_press_test, press_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_grid_weighted = clf_grid_weighted.predict(x5_press_test)\n",
    "print('grid - weighted')\n",
    "print(model_metrics(y5_press_test, press_grid_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_grid_linear_weighted = clf_grid_weighted_linear.predict(x5_press_test)\n",
    "print('grid - linear + weighted')\n",
    "print(model_metrics(y5_press_test, press_grid_linear_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-income",
   "metadata": {},
   "source": [
    "## Top 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "y25 = scaled_training_25['Popular']\n",
    "x25 = scaled_training_25.drop('Popular', axis=1)\n",
    "\n",
    "param_grid = {'C' : np.logspace(-3, 2, 6)}\n",
    "weight_array_25 = weights_25.unique()\n",
    "weight_dict_25 = {0 : weight_array_25[0], 1 : weight_array_25[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default SVM\n",
    "clf_25 = svm.SVC()\n",
    "clf_25.fit(x25, y25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default kernel (rbf), no weights\n",
    "svc_classifier_grid_25 = GridSearchCV(svm.SVC(), param_grid=param_grid)\n",
    "clf_grid_25 = svc_classifier_grid_25.fit(x25,y25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default kernel (rbf), weights\n",
    "svc_classifier_grid_25 = GridSearchCV(svm.SVC(class_weight=weight_dict_25), param_grid=param_grid, n_jobs=8)\n",
    "clf_grid_weighted_25 = svc_classifier_grid_25.fit(x25,y25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear kernel, no weights\n",
    "svc_classifier_grid_25 = GridSearchCV(svm.LinearSVC(), cv=5, param_grid=param_grid, n_jobs=8)\n",
    "clf_grid_linear_25 = svc_classifier_grid_25.fit(x25,y25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear kernel, weights\n",
    "svc_classifier_grid_25 = GridSearchCV(svm.LinearSVC(class_weight=weight_dict_25), param_grid=param_grid, cv=5, n_jobs=8)\n",
    "clf_grid_weighted_linear_25 = svc_classifier_grid_25.fit(x25,y25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-category",
   "metadata": {},
   "source": [
    "#### Prediction + Evaluation for Top 25% models\n",
    "\n",
    "Prediction for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "y25_cnn_test = cnn_scaled_test_25['Popular']\n",
    "x25_cnn_test = cnn_scaled_test_25.drop('Popular', axis=1)\n",
    "\n",
    "cnn_default_25 = clf_25.predict(x25_cnn_test)\n",
    "print('clf - default settings')\n",
    "print(model_metrics(y25_cnn_test, cnn_default_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_grid_25 = clf_grid_25.predict(x25_cnn_test)\n",
    "print('grid - default settings')\n",
    "print(model_metrics(y25_cnn_test, cnn_grid_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_grid_weighted_25 = clf_grid_weighted_25.predict(x25_cnn_test)\n",
    "print('grid - weighted')\n",
    "print(model_metrics(y25_cnn_test, cnn_grid_weighted_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_grid_linear_25 = clf_grid_linear_25.predict(x25_cnn_test)\n",
    "print('grid - linear')\n",
    "print(model_metrics(y25_cnn_test, cnn_grid_linear_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_grid_linear_weighted_25 = clf_grid_weighted_linear_25.predict(x25_cnn_test)\n",
    "print('grid - linear + weighted')\n",
    "print(model_metrics(y25_cnn_test, cnn_grid_linear_weighted_25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-humanity",
   "metadata": {},
   "source": [
    "Prediction for BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "y25_bbc_test = bbc_scaled_test_25['Popular']\n",
    "x25_bbc_test = bbc_scaled_test_25.drop('Popular', axis=1)\n",
    "\n",
    "bbc_default_25 = clf_25.predict(x25_bbc_test)\n",
    "print('clf - default settings')\n",
    "print(model_metrics(y25_bbc_test, bbc_default_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_grid_25 = clf_grid_25.predict(x25_bbc_test)\n",
    "print('grid - default settings')\n",
    "print(model_metrics(y25_bbc_test, bbc_grid_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_grid_weighted_25 = clf_grid_weighted_25.predict(x25_bbc_test)\n",
    "print('grid - weighted')\n",
    "print(model_metrics(y25_bbc_test, bbc_grid_weighted_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_grid_linear_weighted_25 = clf_grid_weighted_linear_25.predict(x25_bbc_test)\n",
    "print('grid - linear + weighted')\n",
    "print(model_metrics(y25_bbc_test, bbc_grid_linear_weighted_25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-laundry",
   "metadata": {},
   "source": [
    "Prediction for Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "y25_reuters_test = reuters_scaled_test_25['Popular']\n",
    "x25_reuters_test = reuters_scaled_test_25.drop('Popular', axis=1)\n",
    "\n",
    "reuters_default_25 = clf_25.predict(x25_reuters_test)\n",
    "print('clf - default settings')\n",
    "print(model_metrics(y25_reuters_test, reuters_default_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_grid_25 = clf_grid_25.predict(x25_reuters_test)\n",
    "print('grid - default settings')\n",
    "print(model_metrics(y25_reuters_test, reuters_grid_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_grid_weighted_25 = clf_grid_weighted_25.predict(x25_reuters_test)\n",
    "print('grid - weighted')\n",
    "print(model_metrics(y25_reuters_test, reuters_grid_weighted_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_grid_linear_weighted_25 = clf_grid_weighted_linear_25.predict(x25_reuters_test)\n",
    "print('grid - linear + weighted')\n",
    "print(model_metrics(y25_reuters_test, reuters_grid_linear_weighted_25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-present",
   "metadata": {},
   "source": [
    "Prediction for PressTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "y25_press_test = press_scaled_test_25['Popular']\n",
    "x25_press_test = press_scaled_test_25.drop('Popular', axis=1)\n",
    "\n",
    "press_default_25 = clf_25.predict(x25_press_test)\n",
    "print('clf - default settings')\n",
    "print(model_metrics(y25_press_test, press_default_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_grid_25 = clf_grid_25.predict(x25_press_test)\n",
    "print('grid - default settings')\n",
    "print(model_metrics(y25_press_test, press_grid_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_grid_weighted_25 = clf_grid_weighted_25.predict(x25_press_test)\n",
    "print('grid - weighted')\n",
    "print(model_metrics(y25_press_test, press_grid_weighted_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_grid_linear_weighted_25 = clf_grid_weighted_linear_25.predict(x25_press_test)\n",
    "print('grid - linear + weighted')\n",
    "print(model_metrics(y25_press_test, press_grid_linear_weighted_25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
